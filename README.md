A comprehensive comparative analysis of traditional Machine Learning and deep learning models for multi-class text classification on a large-scale, well-balanced dataset. Utilized a range of advanced NLP techniques, including custom preprocessing, word vectorization (TF-IDF), and dense word embeddings (GloVe and a self-trained Word2Vec model). Implemented and fine-tuned total 22 models, including Logistic Regression, Naive Bayes, Random Forest(Machine Learning models); Deep Neural Networks (DNN) for sparse and dense inputs, SimpleRNN, GRU, LSTM, and their bidirectional variants (Neural Network Models) .Achieved a 71.46% test accuracy with a GRU model, outperforming the top-performing ML model by over 2.6 percentage points, demonstrating proficiency in advanced deep learning architectures for complex NLP tasks. A report is also given for detailed discussion.

A comprehensive comparative analysis of traditional Machine Learning and deep learning models for multi-class text classification on a large-scale, well-balanced dataset. Utilized a range of advanced NLP techniques, including custom preprocessing, word vectorization (TF-IDF), and dense word embeddings (GloVe and a self-trained Word2Vec model). Implemented and fine-tuned over 20 models, including Logistic Regression, Naive Bayes, and gated recurrent neural networks (GRUs, LSTMs). Achieved a 71.46% test accuracy with a GRU model, outperforming the top-performing ML model by over 2.6 percentage points, demonstrating proficiency in advanced deep learning architectures for complex NLP tasks.
